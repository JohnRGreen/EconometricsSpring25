
% Inbuilt themes in beamer
\documentclass[aspectratio=169]{beamer}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{pgfplots}
\usepgfplotslibrary{fillbetween}

% Theme choice:
\usetheme{CambridgeUS}

% Title page details: 
\title{Probability and Statistics Review \\
        \large Econometrics discussion section 1} 
\author{John Green}
\date{Spring 2025}


\begin{document}

% Title page
\begin{frame}
    \titlepage 
\end{frame}

% housekeeping
\begin{frame}{Housekeeping}
    \begin{itemize}
        \item Introductions: name, major, thoughts about econometrics
        \item For now, office hour in Brody Cafe, Tuesdays, 3-4PM
        \begin{itemize}
            \item May change to Wyman Park 601A
            \item Timing poll: \href{bit.ly/metricsOH}{bit.ly/metricsOH}
        \end{itemize}
        \item What will we do in sections?
        \begin{itemize}
            \item Cover important topics
            \item Practice problems
            \item Stata practice
            \item Slides and resources in GitHub repo: \href{github.com/JohnRGreen/EconometricsSpring25}{github.com/JohnRGreen/EconometricsSpring25}
        \end{itemize}
        \item Sign in for attendance each week
    \end{itemize}
\end{frame}

% Outline frame
\begin{frame}{Recap}
    What have we covered so far?
    \begin{itemize}
        \item Review of probability and statistics (Stock \& Watson chapters 2 and 3)
        \item Random variables and their first 2 moments
        \item Marginal, joint and conditional distributions
        \item Independence
        \item Covariance and correlation
        \item Law of Iterated Expectations
    \end{itemize}
\end{frame}

\begin{frame}{Recap}
    Still to come (probably):
    \begin{itemize}
        \item Normal, Fisher (F), $\chi^2$ distributions      
        \item LLN and CLT
        \item Estimators and their properties: consistency, unbiasedness, normal approximation
        \item One variable t-test
        \item Two variable t-test
    \end{itemize}
\end{frame}

\begin{frame}{Random variables}
    \begin{itemize}
        \item A random variable is a \textit{function} from a space of possible outcomes to (usually) some subset of real numbers.
        \item May be discrete or continuous
        \item Roll two dice and add up the numbers (discrete)
            \begin{itemize}
                \item Sample space is all the possible rolls of the two dice (how many?)
                \item Outcome space is all the possible sums of those rolls (how many?)
                \item The sum is a random variable
            \end{itemize}
        \item Height of a person (continuous, but bounded)
            \begin{itemize}
                \item Sample space is all people; might think about characterizing them by some covariates such as weight, age, etc.
                \item Outcome space is all possible heights (what would this be?)
            \end{itemize}
        \item Thinking about probability is different for discrete and continuous cases
    \end{itemize}
\end{frame}

\begin{frame}
    \centering
    \includegraphics[width = 0.5\textwidth,keepaspectratio]{dice.png}
\end{frame}

\begin{frame}{Mean}
    \begin{itemize}
        \item The first moment of a random variable is its mean, aka average or expected value
        \item What is the mean value of the result from rolling one die?
        \item What is the mean value of the sum of rolling both dice?
    \end{itemize}
\end{frame}

\begin{frame}{Mean}
    \begin{itemize}
        \item The first moment of a random variable is its mean, aka average, aka expected value
        \item What is the mean value of the result from rolling one die?
        \begin{itemize}
            \item $E[X] = \frac{1}{6} \times 1 + \frac{1}{6} \times 2 + \frac{1}{6} \times 3 + \frac{1}{6} \times 4 + \frac{1}{6} \times 5 + \frac{1}{6} \times 6 = 3.5$
        \end{itemize}
        \item What is the mean value of the sum of rolling both dice?
        \begin{itemize}
            \item $E[X] = \frac{1}{36} \times 2 + \frac{2}{36} \times 3 + \frac{3}{36} \times 4 + \frac{4}{36} \times 5 + \frac{5}{36} \times 6 + \frac{6}{36} \times 7 + \frac{5}{36} \times 8 + \frac{4}{36} \times 9 + \frac{3}{36} \times 10 + \frac{2}{36} \times 11 + \frac{1}{36} \times 12 = 7$
            \item (Could also write out using $\frac{1}{36}$ weight everywhere but easier to consolidate terms)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Variance}
    \begin{itemize}
        \item The second moment of a random variable is its variance
        \begin{itemize}
            \item Think of this as the mean distance from the mean: it measures the spread of our data
            \item $Var(X) = E \left[ (X - E[X])^2 \right] $
            \item Why do we need to square the spread?
        \end{itemize}
        \item What is the variance of rolling one die?
        \item What is the variance of the sum of rolling both dice?
    \end{itemize}
\end{frame}

\begin{frame}{Variance}
    \begin{itemize}
        \item The second moment of a random variable is its variance
        \begin{itemize}
            \item Think of this as the mean distance from the mean: it measures the spread of our data
            \item $Var(X) = E \left[ (X - E[X])^2 \right] $
            \item Why do we need to square the spread?
        \end{itemize}
        \item What is the variance of rolling one die?
        \begin{itemize}
            \item $Var(X) = \frac{1}{6} \times (1 - 3.5)^2 + \frac{1}{6} \times (2 - 3.5)^2 + \dots + \frac{1}{6} \times (6 - 3.5)^2 = \frac{35}{12} \approx 2.92$
        \end{itemize}
        \item Similar idea for the sum of 2 dice.
        \item Often we will work with the \textit{standard deviation} which is the square of the variance since the units are more interpretable.
    \end{itemize}
\end{frame}

\begin{frame}{Higher moments}
    \begin{itemize}
        \item Third moment is \textit{skewness} and tells us how symmetric our distribution is
        \begin{itemize}
            \item What is the skewness of our example?
        \end{itemize}
        \item Fourth moment is the \textit{kurtosis} which measures the mass of the tails
        \begin{itemize}
            \item Gives us an idea of the likelihood of large values
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Covariance}
    \begin{itemize}
        \item The covariance of two random variables tells us the strength of their \textit{linear} (careful!) relationship
        \item If two random variables are independent, covariance is 0 (though the reverse is not true)
        \begin{itemize}
            \item What is the covariance in our example?
        \end{itemize}
        \item Often we will look at the \textit{correlation} instead of the covariance, $\frac{cov(X,Y)}{(var(X)var(Y))^{1/2}}$
        \begin{itemize}
            \item Unlike covariance, correlation is scaled between -1 and 1 and so is easily interpretable
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Rules on joint distributions}
        \begin{itemize}
            \item If two variables are independent, $P(X=x,Y=y) = P(X=x)*P(Y=y)$
            \item Conditional probability: $P(X=x|Y=y) = \frac{P(X=x,Y=y)}{P(Y=y)}$
            \begin{itemize}
                \item What happens if the events are independent?
            \end{itemize}
            \item Law of iterated expectations says the mean of $Y$ can be written as a weighted average of the mean of $Y|X$: $E[Y] = E[E[Y|X=x]]$
            \begin{itemize}
                \item $E[Y] = \sum_x E[Y|X=x]P(X=x)$ in discrete case
                \item $E[Y] = \int_x E[Y|X=x]f_X(x)dx$ for continuous case
            \end{itemize}
            \item Other topics in textbook: Bayes' law, law of total probability, etc.
        \end{itemize}
\end{frame}

\begin{frame}{Second example}
    \begin{itemize}
        \item New example: 
        \begin{itemize}
            \item Roll the first dice
            \item If first roll $\geq 4$ then we roll the second die and observe its value
            \item If first roll $\leq 3$ then the second value is simply set to 1
        \end{itemize}
        \item What is the joint probability distribution?
        \item What is the covariance of the 2 rolls?
        \item What is the correlation?
        \item What is the marginal distribution of the two r.v.? What is the first moment of each?
    \end{itemize}
\end{frame}

\begin{frame}
    \centering
    \includegraphics[width = 1\textwidth,keepaspectratio]{marginal.png}
\end{frame}

\begin{frame}{Second example}
    \begin{itemize}
        \item New example: 
        \begin{itemize}
            \item Roll the first dice
            \item If first roll $\geq 4$ then we roll the second die and observe its value
            \item If first roll $\leq 3$ then the second value is simply set to 1
        \end{itemize}
        \item What is the marginal distribution of the two r.v.? What are the first two moments of each? $E[X_1] = 3.5$, $var(X_1) = 2.92$, $E[X_2] = 2.25$, $var(X_1) = 3.02$
        \item What is the covariance of the 2 rolls? $cov(X_1,X_2) = 1.875$
        \item What is the correlation? $corr(X_1,X_2) = \rho_{X_1,X_2} = 0.63$
    \end{itemize}
\end{frame}

\begin{frame}{Normal distribution}
    \begin{itemize}
        \item Two parameters: mean $\mu$ and variance $\sigma^2$
        \begin{itemize}
            \item $\mu$ tells us about the location of the distribution (where is it centered?)
            \item $\sigma$ tells us about the shape of the distribution (what is the spread? what do the tails look like?)
        \end{itemize}
        \item We can \textit{standardize} a normal random variable X:
        \begin{itemize}
            \item $Z = \frac{X-\mu}{\sigma}$ so $Z \sim N(0,1)$
            \item Careful to divide by standard deviation and not by variance!
        \end{itemize}
        \item This will be helpful in hypothesis testing because we can consider how unlikely a given value $z$ is to be drawn from a N(0,1) distribution
    \end{itemize}
\end{frame}

\begin{frame}{Estimation}
    \begin{itemize}
        \item So far we have been in the world of probability theory, dealing with random variables and their distributions
        \item \textit{Statistics} is all about using actual data to make inferences about the world, and so we will often try to find \textit{estimators} for probabilistic objects
        \item For example, we will often be interested in estimating the mean (average, first moment) of a distribution
    \end{itemize}
\end{frame}

\begin{frame}{Estimation}
    \begin{itemize}
        \item Example: wait time for a coffee at Brody Cafe
        \item A natural estimator for the population mean is the sample mean:
        \begin{itemize}
            \item Take a survey of people as they walk out of Brody and ask how long they waited
            \item Average the responses
        \end{itemize}
        \item Sample mean $\bar{Y}$ is a random variable since we taking a random sample and thus $\bar{Y}$ has a sampling distribution
        \begin{itemize}
            \item What can we say about it?
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Unbiased}
    \begin{itemize}
        \item First, $\bar{Y}$ is an \textbf{unbiased} estimator of $\mu_Y$:
        
        $$
        E[\bar{Y}] = \mu_Y
        $$

        \item In expectation, we get the thing we want!
    \end{itemize}
\end{frame}

\begin{frame}{Law of Large Numbers}
    \begin{itemize}
        \item Second, $\bar{Y}$ is a \textbf{consistent} estimator of $\mu_Y$
        \item Mathematically, this means that for any number $\epsilon$ we choose, we know that
        $$
            \text{lim}_{n \to \inf} P(|\bar{Y} - \mu_Y| > \epsilon) = 0
        $$
        \item In plain English: the larger our sample size, the better our guess for $\mu_Y$
        \item As our sample gets infinitely large, we get infinitesimally close
        \item This follows from the \textbf{Law of Large Numbers}
    \end{itemize}
\end{frame}

\begin{frame}
    \centering
    \includegraphics[width = .8\textwidth,keepaspectratio]{LLN.png}
\end{frame}

\begin{frame}{Efficiency}
    \begin{itemize}
        \item Lastly, we can think about the \textit{efficiency} of the estimator: what is its variance?
        \item In this case, $var(\bar{Y}) = \frac{\sigma_Y^2}{n}$
        \item In other words, the variance decreases as the sample gets larger, at a rate of $\frac{1}{n}$
        \item We can use this to compare the efficiency of two estimators, for example if we had another estimator with variance $\frac{\sigma_Y^2}{\sqrt{n}}$ we can say that the sample mean is more efficient
    \end{itemize}
    Now let's look at the most remarkable result about the sample mean.
\end{frame}

\begin{frame}{Central Limit Theorem}
    \begin{itemize}
        \item The central limit theorem tells us that $\bar{Y}$ is \textit{asymptotically normal}: 
        $$
            \sqrt{n} (\bar{Y} - \mu_Y) \xrightarrow{d} N(0,\sigma_Y^2)
        $$
        \item In other words, as $n \to \infty$, $\bar{Y}$ follows a normal distribution, $\bar{Y} \sim N(\mu_Y,\frac{\sigma_Y^2}{n})$
        \item This means we can use the normal distribution to make inferences about sample mean
        \item To make it easy, we can standardize the sample mean: $Z = \frac{\bar{Y} - \mu_Y}{\sigma_Y/\sqrt{n}} \sim N(0,1)$
        \item We will use the sample variance as an estimator for the population variance, just like we do for mean (but we will need to correct a small bias)
    \end{itemize}
\end{frame}

\begin{frame}{Hypothesis testing}
    \begin{itemize}
        \item We can use the normal approximation to perform a \textit{hypothesis test}
        \begin{itemize}
            \item Set a null hypothesis, eg $\mu_Y = \mu_{Y,0}$
            \item Usually we make the null the thing we "want" to disprove; if we reject the null, then we have evidence for our alternative hypothesis, eg $\mu_Y \neq \mu_{Y,0}$
        \end{itemize}
        \item Intuition: assuming the true mean is some value $\mu_{Y,0}$, how likely is it that we would observe the sample mean $\bar{Y}$?
        \begin{itemize}
            \item If it is ``very'' unlikely, we will reject the null
            \item If it is ``reasonably likely'' then we fail to reject the null
        \end{itemize}
        \item Alternative perspective: what would I have to see to convince myself \textit{that my hypothesis is wrong}?
        \item Usually, we standardize everything and create a $t-statistic$
        \begin{itemize}
            \item We test $t$ under the null that $t \sim N(0,1)$ rather than work directly with the sample mean
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Hypothesis testing (cont.)}
    \begin{itemize}
        \item Set a given significance $\alpha$ and find the \textit{critical value(s)}
        \begin{itemize}
            \item ie the value(s) such that the sum in the tails (or tail) of the standard normal distribution is $\alpha$
        \end{itemize}
        \item A common value is $\alpha=0.05$, with critical values $c_1=-1.96$ and $c_2 = 1.96)$
        \item In this test, the probability of getting a $t$ less than $-1.96$ or greater than $1.96$ is 0.05
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{2-tailed test with $\alpha$ = 0.05}
    \begin{center}
    \begin{tikzpicture}[scale=0.9]
    \begin{axis}[
        axis lines=middle,
        % xlabel=$z$,
        % ylabel=$f(z)$,
        % xlabel style={at={(axis description cs:1,-0.1)},anchor=west},
        % ylabel style={anchor=south},
        xtick={-3,-2,-1,0,1,2,3},
        xmin=-3.5,
        xmax=3.5,
        ymin=0,
        ymax=0.45,
        samples=100,
        clip=false
    ]
    
    % Define the normal distribution curve
    \addplot[name path=curve, thick, blue, domain=-3.5:3.5] {1/sqrt(2*pi)*exp(-x^2/2)};
        
    % Define the x-axis baseline
    \addplot[name path=axis, domain=-3.5:3.5] {0};

    % Fill the area to the left of -1.96
    \addplot[blue!30, fill opacity=0.5] fill between[of=curve and axis, soft clip={domain=-3.5:-1.96}];

    % Fill the area to the right of 1.96
    \addplot[blue!30, fill opacity=0.5] fill between[of=curve and axis, soft clip={domain=1.96:3.5}];

    % Normal distribution curve again (to make it visible over shading)
    % \addplot[thick, blue, domain=-3.5:3.5] {1/sqrt(2*pi)*exp(-x^2/2)};
    
    % Shaded area to the left of z = -0.5
    % \addplot[blue!30, fill] fill between[of=curve and axis, soft clip={domain=-3.5:-0.5}];
        
    % Compute the function value at z = -1.96
    \pgfmathsetmacro{\fxa}{1/sqrt(2*pi)*exp(-1.96^2/2)}

    % Markers for critical values
    \addplot[red, thick] coordinates {(-1.96,0) (-1.96,\fxa)};
    \node[above] at (axis cs:-1.7, 0) {$c_1$};
    
    % % Compute the function value at z = 1.96
    \pgfmathsetmacro{\fxb}{1/sqrt(2*pi)*exp(-1.96^2/2)}

    % Markers for critical values
    \addplot[red, thick] coordinates {(1.96,0) (1.96,\fxb)};
    \node[above] at (axis cs:1.7, 0) {$c_2$};

    \end{axis}
    \end{tikzpicture}
    \end{center}
\end{frame}

\begin{frame}{Hypothesis testing (cont.)}
    \begin{itemize}
        \item Set a given significance $\alpha$ and find the \textit{critical value(s)}
        \begin{itemize}
            \item ie the value(s) such that the sum in the tails (or tail) of the standard normal distribution is $\alpha$
        \end{itemize}
        \item A common value is $\alpha=0.05$, with critical values $c_1=-1.96$ and $c_2 = 1.96)$
        \item In this test, the probability of getting a $t$ less than $-1.96$ or greater than $1.96$ is 0.05
        \item Our decision procedure is that we reject (fail to accept) the null hypothesis if $|t| > 1.96$
        \begin{itemize}
            \item Intuition: under the null, it is very unlikely we observe a $t$ in the tails. Thus we reject the null if we do so.
        \end{itemize}
        \item Alternatively, if $|t| < 1.96 $ then we fail to reject $H_0$
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Standard Normal Distribution}
    \begin{center}
    \begin{tikzpicture}[scale=0.8]
    \begin{axis}[
        axis lines=middle,
        % xlabel=$z$,
        % ylabel=$f(z)$,
        % xlabel style={at={(axis description cs:1,-0.1)},anchor=west},
        % ylabel style={anchor=south},
        xtick={-3,-2,-1,0,1,2,3},
        xmin=-3.5,
        xmax=3.5,
        ymin=0,
        ymax=0.45,
        samples=100,
        clip=false
    ]
    
    % Define the normal distribution curve
    \addplot[name path=curve, thick, blue, domain=-3.5:3.5] {1/sqrt(2*pi)*exp(-x^2/2)};
        
    % Define the x-axis baseline
    \addplot[name path=axis, domain=-3.5:3.5] {0};

    % Fill the area to the left of z = -0.5
    \addplot[blue!30, fill opacity=0.5] fill between[of=curve and axis, soft clip={domain=-3.5:-0.5}];

    % Normal distribution curve again (to make it visible over shading)
    % \addplot[thick, blue, domain=-3.5:3.5] {1/sqrt(2*pi)*exp(-x^2/2)};
    
    % Shaded area to the left of z = -0.5
    % \addplot[blue!30, fill] fill between[of=curve and axis, soft clip={domain=-3.5:-0.5}];
        
    % Compute the function value at z = -0.5
    \pgfmathsetmacro{\fx}{1/sqrt(2*pi)*exp(-0.5^2/2)}

    % Marker at z = -0.5
    % \draw[red, thick] (-0.5,0) -- (-0.5,\fx);
    \addplot[red, thick] coordinates {(-0.5,0) (-0.5,\fx)};
    \node[below] at (axis cs:-0.5, 0) {$z$};
    
    \end{axis}
    \end{tikzpicture}
    \end{center}
\end{frame}

\begin{frame}{Calculating p-value}
    \begin{itemize}
        \item Alternatively we might focus on the \textit{p-value}: probability of a test statistic at least as unlikely as the one you observe (under the null)
            $$ P(x > |t|) $$
        \item So, the \textbf{p-value} tells us how unlikely the null hypothesis is:
        \begin{itemize}
            \item  $P_{H_0}(|\frac{\bar{Y} - \mu_{Y,0}}{\sigma_Y/\sqrt{n}}|\geq |\frac{\bar{Y}_{data} - \mu_{Y,0}}{\sigma_Y/\sqrt{n}}|)$
            \item RHS is just a number! LHS is RV with known distribution under null
            \item CLT tells us this \textbf{t-statistic} is N(0,1) so probability in tails is easy to look up
        \end{itemize}
        \item If observed t-statistic is very large or very small, then p-value is very small and we reject the null
        \begin{itemize}
            \item If the null had been true, it is very unlikely that we would have observed the t-statistic
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Confidence interval}
    \begin{itemize}
        \item Comparing p-value to $\alpha$ is equivalent to comparing $t$ to $c$ 
        \item In either case, more rigorous to pre-set our rejection region
        \item We can also construct a \textit{confidence interval} for $\mu_X$ based on our data: the set of all possible $\mu_X$ values which would not be rejected by a two-sided t-test with size $\alpha$
            $$\bar{X} \pm c(\alpha) \frac{\sigma_X}{\sqrt{n}} = \left( -c(\alpha) \frac{\sigma_X}{\sqrt{n}}, c(\alpha) \frac{\sigma_X}{\sqrt{n}}\right)$$
        \item Under the null, 95\% of the time we draw a value in this interval
        \item What would this look like for a one-sided test?
    \end{itemize}
\end{frame}


\begin{frame}{Some terminology}
    \begin{itemize}
        \item \textbf{Type-1 error}: reject a true null hypothesis
        \begin{itemize}
            \item \textbf{Size} is probability of a type-1 error
        \end{itemize}
        \item \textbf{Type-2 error}: fail to reject a false hypothesis
        \begin{itemize}
            \item \textbf{Power} is probability of a type-2 error
        \end{itemize}
        \item Which of these two mistakes is worse?
        \item What is the relationship between the size and the power?
    \end{itemize}
\end{frame}

\begin{frame}
    \centering
    \includegraphics[width = .6\textwidth,keepaspectratio]{size_and_power.png}
\end{frame}


\begin{frame}{Hypothesis about difference in means}
    \begin{itemize}
        \item What if we want to perform a hypothesis on the difference in means between two random variables? For example:
        $$H_0 : \mu_1 = \mu_2 $$
        \item Simple: combine them to one!
        \begin{itemize}
            \item $\mu^* = \mu_1 = \mu_2$
            \item This is just a difference of two normal random variables so we can use the normal approximation, t-statistic is:
            
            $$t = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}$$
        \end{itemize}
        \item What if we want to test hypothesis that the difference is 2?
    \end{itemize}
\end{frame}

\end{document}